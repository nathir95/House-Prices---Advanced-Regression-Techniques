from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, cross_val_score
from sklearn.tree import DecisionTreeRegressor
import xgboost as xgb

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_squared_log_error


y= data_train['SalePrice']
X = data_train.drop('SalePrice', axis=1)



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)



lr = LinearRegression(normalize= True)
lr.fit(X,y)


predictions = lr.predict(X_test)


rmse = mean_squared_error(y_test, predictions)**0.5
rmse












n_folds = 5 

def rmse_cv(model):
    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)
    
    rmse = np.sqrt(cross_val_score(model, train.values, y, scoring= "neg_mean_squared_error", cv = kf))
    return(rmse)

model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, min_child_weight=1.7817, n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213, random_state=42)

model_xgb.fit(train,y)

rmse_cv(model_xgb)

test_pred = np.expm1(model_xgb.predict(test))

print(test_pred)
