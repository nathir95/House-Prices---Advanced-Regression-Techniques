from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, cross_val_score
from sklearn.tree import DecisionTreeRegressor
import xgboost as xgb

n_folds = 5 

def rmse_cv(model):
    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)
    
    rmse = np.sqrt(cross_val_score(model, train.values, y, scoring= "neg_mean_squared_error", cv = kf))
    return(rmse)

model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, min_child_weight=1.7817, n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213, random_state=42)

model_xgb.fit(train,y)

rmse_cv(model_xgb)

test_pred = np.expm1(model_xgb.predict(test))

print(test_pred)
